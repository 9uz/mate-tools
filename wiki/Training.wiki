# This page describes the training of the tagger, lemmatizer, morphologic tagger and dependency parser

= Traing of a tagger, lemmatizer, morphologic tagger, and dependency parser =

All the analyser tools have similar command line options. Some options are only applicable to some tools 

`java -Xmx4G -classpath anna-<version>.jar <class>` 
`[-model <model>]` 
`[-train <training-corpus>]`  
`[-test <test-file>]`  
`[-out <result-file>]`  
`[-eval <evaluation-file>]`
`[-i <trainig-iterations>]`
`[-count <sentences-used>]`
`[-hsize <weight-vector-size>]`

Additional parser options:
`[-decode proj]`
`[-cores <number-of-cores>]`

The Parameters are:
 * <version>the version of the jar file (see download section)
 * <class> The class and path of the tool:  `is2.parser.Parser` | `is2.tag3.Tagger` | `is2.lemmatizer.Lemmatizer` | `is2.mtag.Main` 
 * <model> The name of a model which is created during the traing or used by tool for tagger, parsing, etc.
 * <training-corpus> A corpus that is used for training in [http://ufal.mff.cuni.cz/conll2009-st/task-description.html#Dataformat CoNLL 2009 format]
 * <test-file> The file contains the sentences that the tools has to process
 * <result-file> The output of the tools in CoNLL 09 format
 * <evaluation-file>A file the output is compared to  
 * <training-iterations> number training rounds; typically 10
 * <count> the first n sentences are used only for training
 * <weight-vector-size> the size of the weight vector 
 * <number-of-cores> the number of maximal employed CPU cores
 

===Examples===

Training of a tagger:

`java -Xmx4G  -classpath anna-1.jar is2.tag3.Tagger  -model models/tag-v1-eng.model  -train connl_09_st_eng_train/CoNLL2009-ST-English-train.txt `

Training, Testing, and Evaluation of the tagger:

`java -Xmx4G  -classpath anna-1.jar is2.tag3.Tagger  -model models/tag-v1-eng.model  -train connl_09_st_eng_train/CoNLL2009-ST-English-train.txt  -test gold/CoNLL2009-ST-evaluation-English.txt  -out results/tagged-eng.txt  -eval gold/CoNLL2009-ST-evaluation-English.txt`

== Training Time and Memory Consumption ==

Typical training time is about 10-12 hours (a night) on a fast 4 core machine (Intel Nehalem, 3.33 Ghz) for English.  The same training times is needed on a slower 2.2 Ghz, 8 core AMD system. The model takes about 100MB of disk space. The training times for other languages are quite different. On the Chinese corpus, the training requires the most time and resources because of many long sentences uses. For English, 4GB free memory and for Chinese about 11GB main memory is needed.  
